<!DOCTYPE html>
<html lang="en">
<head>
    <title>Database Saturday</title>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta property="og:image" content="pictures/preview.jpg">
    <meta property="og:title" content="Database Saturday">
    <meta property="og:type" content="website">
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:image" content="pictures/preview.jpg">
    <link rel="stylesheet" href="shower/themes/yandex/styles/screen-16x9.css">

    <style type="text/css">
         code { display: block; white-space: pre; background-color: #EEE; }
    </style>
</head>
<body class="shower list">
    <header class="caption">
        <h1>Database Saturday</h1>
    </header>
    <section class="slide" id="cover">
        <h1 style="margin-top: 150px; font-size: 32pt;"><div>How to Stop Being Afraid</div><div style="text-align: right;">and Start Developing</div><div style="text-align: right;">Specialized Data Structures</div></h1>
    </section>

    <section class="slide">
        <h2>Why?</h2>
<p>Why write specialized data structures yourself?</p>

<p>&mdash; because we can?<br/>
&mdash; just felt like it?<br/>
&mdash; NIH (not invented here).</p>

<p>It's better to use instead<br/><br/>... reliable, modern, scalable, fault-tolerant, supported, time-tested, community-approved technologies, developed by the best engineers, who wrote perfect code that solves exactly the task you need.</p>
    </section>


    <section class="slide">
        <h2>Realtime Anti-fraud</h2>

<p>For classifying website visits as human / robot.</p>

<p>Developed in <b>2012</b> for Yandex.Metrica needs.</p>

<p>I can neither confirm nor deny<br/> the fact that it's still running in production.</p>
    </section>


    <section class="slide">
        <h2>Realtime Anti-fraud</h2>

<p>A set of servers, we send information about traffic to them.</p>
<p>Servers update and provide statistics on traffic properties.</p>
<p>A machine-learned formula works on the statistics.</p>
    </section>


    <section class="slide">
        <h2>What Traffic Properties Exist</h2>

<p>&mdash; Event time</p>
<p>&mdash; IP address, IP network, geolocation;</p>
<p>&mdash; User cookie;</p>
<p>&mdash; URL and Referer, as well as their domains;</p>
<p>&mdash; User-Agent;</p>
<p>...</p>
    </section>


    <section class="slide">
        <h2>What Statistics Can Be Calculated</h2>

<p>Counters &mdash; number of events by key:
<br/>&mdash; for example &mdash; number of hits for a class C IP network per minute.</p>

<p>Cardinalities &mdash; number of unique values for a key:
<br/>&mdash; example: number of unique cookies for an IP address;
<br/>&mdash; example: number of unique IP addresses for a cookie;
<br/>&mdash; example: number of unique sites visited by a user per hour.</p>

<p>Time statistics:
<br/>&mdash; example: variance of the difference between adjacent events.</p>
    </section>


    <section class="slide">
        <h2>Arithmetic: Throughput</h2>

<p>Incoming traffic 600,000 events per second,<br/> &mdash; 30 billion events per day.</p>
<p>For each event, we update statistics for 15 keys<br/> &mdash; 10 million key lookups/sec.</p>

<p>What hardware to use for 10 million lookups/sec.?</p>
    </section>


    <section class="slide">
        <h2>Arithmetic: Throughput</h2>

<p><b>HDD</b> &mdash; 100..300 lookups/sec.<br/>Would need 100,000 HDDs without redundancy or ~10,000 servers.</p>

<p><b>SSD</b> &mdash; ~100,000 lookups/sec.<br/>Would need 100 SSDs without redundancy or 10 servers?
<br/>&mdash; but cross out, since read/write ratio is 1:1.</p>

<p><b>RAM</b> &mdash; 10 million / 40 million / 500 million LL cache misses / sec.</p>
    </section>


    <section class="slide">
        <h2>Arithmetic: Volume</h2>

<p>Want to store data for 1..2 days.<br/>
The fattest keys &mdash; UserID, URL, Referer, IP.</p>

<p>Cardinalities:
<br/>&mdash; URL &mdash; 1.5 billion per day.
<br/>&mdash; UserID &mdash; 450 million per day;
<br/>&mdash; IP &mdash; 100 million per day;</p>
    </section>

    <section class="slide">
        <h2>Arithmetic: Volume</h2>
<p>Total ~5 billion keys.
<br/>Statistics per key &mdash; around 1 KB.
<br/>&mdash; 5 TB per day.</p>

<p>In 2012, 128 GB of RAM was used per server.
<br/>&mdash; 40 servers without redundancy.</p>
    </section>


    <section class="slide">
        <h2 style="font-size: 30pt;">How to Fit Statistics per Key in 1 KB?</h2>

<p>Even a single URL can be several kilobytes.</p>
<p>&mdash; never store strings, only 8-byte hashes.</p>
    </section>


    <section class="slide">
        <h2 style="font-size: 30pt;">How to Fit Statistics per Key in 1 KB?</h2>
<p>Need to calculate very poor statistics.</p>

<p>1. Counters.</p>

<code style="margin-bottom: 1em;">UInt64 count = 0;
void update() { ++count; }</code>

<p>8 bytes &mdash; horribly too much.</p>
<p>Can we count from one to a billion using one byte?</p>
    </section>

    <section class="slide">
        <h2 style="font-size: 30pt;">How to Fit Statistics per Key in 1 KB?</h2>

<p>Can we count from one to a billion using one byte?</p>

<p>Yes &mdash; you just need to use a random number generator.</p>

<p>&mdash; for counter &lt; 8, add one with probability 1.
<br/>&mdash; for counter [8..16), add one with probability 1/2.
<br/>&mdash; for counter [16..24), add one with probability 1/4.
<br/>&mdash; ...
<br/>&mdash; for counter [128..256), add one with probability 1/2<sup>31</sup>.</p>

<p>The expected value is estimated<br/>by the maximum likelihood method.</p>
<p>The method can be either non-deterministic or deterministic, if you use a hash function on the data instead of a random number generator.</p>
    </section>


    <section class="slide">
        <h2 style="font-size: 30pt;">How to Fit Statistics per Key in 1 KB?</h2>
<p>2. Cardinalities.</p>

<p style="line-height: 1.5em;">Obviously, we need to use <b>HyperLogLog</b>.<br/>
2.5 KB &mdash; ~1% error &mdash; too good, need worse.<br/>
24 bytes &mdash; ~50% error &mdash; now that's normal.</p>

<p>Can already fit 50 cardinalities in 1 KB.</p>
    </section>


    <section class="slide">
        <h2>Too Much Data</h2>

<p>5 TB per day &mdash; <span style="color: red;">50 servers</span> without redundancy.<br/>
&mdash; but need x2 replication for fault tolerance;<br/>
&mdash; but need to store data slightly more than a day.</p>

<p>We don't have 100 servers. <span style="color: red;">Task is unsolvable</span>.</p>
    </section>


    <section class="slide">
        <h2>Too Much Data</h2>

<p>Solution &mdash; don't store statistics for keys<br/>that occur rarely.</p>
<p>If we see an IP address for the first time &mdash; don't store anything about it.<br/>
If we encounter it 16 times &mdash; start collecting statistics.</p>

<p>But to know that it was encountered for the sixteenth time<br/>&mdash; we need to store this somewhere.</p>
    </section>


    <section class="slide">
        <h2>Too Much Data</h2>

<p>Solution &mdash; <b>Counting Bloom Filter</b>.<br/>
Out of 128 GB per server, allocate 10..20 GB for CBF.</p>

<p>CBF works like a &laquo;sponge&raquo; through which only<br/>important keys are filtered.</p>

<p>Remaining data volume decreases to hundreds of GB<br/>and fits on one server.</p>
    </section>


    <section class="slide">
        <h2>Counting Bloom Filter</h2>

<p>Disadvantage &mdash; now if we use 3 hash functions, we'll have to do 3 times more cache misses.</p>
<p>10 million per second -> 30 million per second.</p>

<p>This can still work... even on one server.</p>

<p>Or invent a <b>cache-local</b> Counting Bloom Filter?</p>
    </section>


    <section class="slide">
        <h2>How to Send Data to the Server?</h2>

<p>Suppose the server should handle<br/>~1 million requests per second.</p>
<p>What technologies to use to write such a server?</p>

<p>&mdash; coroutines/fibers?<br/>
&mdash; DPDK?</p>
    </section>


    <section class="slide">
        <h2>How to Send Data to the Server?</h2>
<img style="float: left; height: 60%; margin-left: -60px; margin-top: -60px; margin-right: 20px;" src="pictures/optimal.webp"/>
<p>We have realtime anti-fraud, but we'll send data<br/>in batches of 1,000 .. 10,000 events anyway.</p>
<p>One second delay is acceptable,<br/>and there are 600,000 events per second.</p>

<p><br/>We'll use a regular HTTP server with a thread pool.</p>
    </section>


    <section class="slide">
        <h2>Concurrent Request Processing</h2>
<p>How to ensure concurrent processing<br/>of simultaneously incoming requests?</p>

<p>Maybe use lock-free data structures?</p>
    </section>


    <section class="slide">
        <h2>Concurrent Request Processing</h2>
<img style="float: right; height: 60%; margin-left: -60px; margin-top: -60px; margin-right: 20px;" src="pictures/not_optimal.webp"/>
<p>How to ensure concurrent processing<br/>of simultaneously incoming requests?</p>
<p>Maybe use lock-free<br/>data structures?</p>

<p><br/>No, better to have one global <b>mutex</b> in the server,&nbsp;and process one request at a time<br/>while others wait.</p>
<p>Trading <b>latency</b> for <b>throughput</b>.</p>
    </section>


    <section class="slide">
        <h2>How to Parallelize Processing?</h2>
<p>How to parallelize request processing across CPU cores?</p>

<p>Split all data by remainder of key hash divided by N into buckets.</p>
<p>All data structures in the server (Counting Bloom Filter, Hash Tables) exist in N instances.</p>
<p>Incoming data batch is passed to a thread pool for processing, each thread processes its own keys.</p>

<p>&mdash; internal sharding.</p>
    </section>

    <section class="slide">
        <h2>How to Shard Data</h2>
<p>between servers?</p>
    </section>

    <section class="slide">
        <h2>How to Shard Data?</h2>

<p>&mdash; no way :(</p>
    </section>


    <section class="slide">
        <h2>How to Replicate Data?</h2>

<p>Each server writes a request log to disk and allows the replica to read and process this log.</p>
<p>This same log is used for recovery after failure.</p>
<p>Replication is asynchronous <b>eventually inconsistent</b>.</p>
    </section>


    <section class="slide">
        <h2>How to Delete Old Data?</h2>

<p>Three options:</p>
<p>&mdash; split all data into buckets by hour and delete old buckets;</p>
<p>&mdash; exponential smoothing: periodically divide counter values in half;</p>
<p>&mdash; periodically dump to disk, restart server and read from dump only current data.</p>
    </section>


    <section class="slide">
        <h2>Network Traffic</h2>

<p>Input: one event &mdash; 50 columns UInt64 &mdash; 400 bytes<br/>
~ <span style="color: red;">4 GBit/sec</span>.</p>

<p>Output:
<br/>&mdash; transmit all calculated statistics<br/>&mdash; 500 columns UInt32 &mdash;
2 KB ~ <span style="color: red;">20 GBit/sec</span>.
<br/><br/>&mdash; transmit only machine-learned formula result<br/>&mdash; float &mdash; 4 bytes per event.</p>
    </section>


    <section class="slide">
        <h2>Network Traffic</h2>

<p>But we have a <span style="color: red;">1 GBit</span> network :(</p>

<p>Just compress the data.</p>

<p><b>LZ4</b> &mdash; too weak.<br/>
<b>QuickLZ</b> &mdash; for 2012, there was no better alternative,<br/>now not relevant.<br/>
2019 &mdash; use <b>ZSTD</b> or <b>Brotli</b>.</p>
    </section>


    <section class="slide">
        <h2>Robot Criterion</h2>
<p>How to make decisions about traffic being robotic?</p>

<p>Machine-learned <b>MatrixNet</b> formula.</p>
<p>Now more advanced technology is available<br/>in open-source: <b>CatBoost</b>.</p>

<p><a href="https://github.com/catboost/">https://github.com/catboost/</a></p>
    </section>

    <section class="slide">
        <img src="pictures/perf.png" style="width: 120%; margin: 0 -10% 0 -10%;" />
    </section>

    <section class="slide">
        <h2>Alternatives</h2>

<p>&mdash; Redis;
<br/>&mdash; Aerospike;
<br/>&mdash; Couchbase;
<br/>&mdash; Cassandra.</p>

<p>Bonus:</p>

<p>&mdash; YT Dynamic Tables;
<br/>&mdash; RTMR;
<br/>&mdash; YDB.</p>
    </section>


    <section class="slide">
        <h2>How to Stop Being Afraid?</h2>

<p>Have a good understanding of hardware capabilities.
<br/>Have a good understanding of task properties and its numerical characteristics.
<br/>Have a good understanding of internal structure of available data stores.</p>

<p>Confidence and courage.</p>

<p>Use arithmetic to relate<br/>task properties with hardware capabilities.
<br/><br/>Be ready to investigate when theory doesn't match practice.</p>
    </section>


    <section class="slide">
        <h2>Next</h2>

<p>Visitor history database.</p>
<p>Visitor session calculation.</p>
<p>&mdash; specialized data structures on SSD+RAM.</p>
    </section>


    <section class="slide">
        <h2>?</h2>
    </section>

    <div class="progress"></div>
    <script src="shower/shower.min.js"></script>
</body>
</html>
