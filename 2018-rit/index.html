<!DOCTYPE html>
<html lang="en">
<head>
    <title>ClickHouse Features for Advanced Developers</title>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="shower/themes/yandex/styles/screen-16x9.css">

    <style type="text/css">
         code { display: block; white-space: pre; background-color: #EEE; }
    </style>
</head>
<body class="shower list">
    <header class="caption">
        <h1>ClickHouse Features for Advanced Developers</h1>
    </header>
    <section class="slide" id="cover" style="background: #FFF url('pictures/title.png') no-repeat; background-size: 100%;">
        <h1 style="margin-top: 50px; margin-left: -40px; line-height: 150%">ClickHouse Features<br/>for Advanced Developers</h1>
    </section>

    <section class="slide">
        <h2>Sampling Key</h2>
        <p>You have a clickstream<br/>and you store it in unaggregated form.</p>
        <p>You need to generate reports for clients on the fly.</p>
        <p>This is a typical example of ClickHouse usage.</p>
    </section>

    <section class="slide">
        <h2>Sampling Key</h2>
        <p>Most clients are small,<br/>but there are some really large ones.</p>
<p>You need to get reports "instantly" even for the largest clients.</p>
<p>Solution: declare a SAMPLE key in MergeTree family tables.</p>
    </section>

    <section class="slide">
        <h2>Sampling Key</h2>
        <code style="line-height: 150%;">CREATE TABLE ... ENGINE = MergeTree
ORDER BY (CounterID, Date, <b>intHash32(UserID)</b>)
PARTITION BY toYYYYMM(Date)
SAMPLE BY <b>intHash32(UserID)</b></code>
    </section>

    <section class="slide">
        <h2>Sampling Key</h2>
        <code>SELECT uniq(UserID) FROM hits_all
WHERE CounterID = 76543
  AND EventDate BETWEEN '2018-03-25' AND '2018-04-25'

┌─uniq(UserID)─┐
│     47362335 │
└──────────────┘

1 rows in set. Elapsed: <b>4.571 sec</b>.
Processed 1.17 billion rows, <b>16.37 GB</b>
(255.88 million rows/s., 3.58 GB/s.)</code>
    </section>

    <section class="slide">
        <h2>Sampling Key</h2>
        <code>SELECT uniq(UserID) FROM hits_all
<b>SAMPLE 1/10</b>
WHERE CounterID = 76543
  AND EventDate BETWEEN '2018-03-25' AND '2018-04-25'

┌─uniq(UserID)─┐
│      4742578 │
└──────────────┘

1 rows in set. Elapsed: <b>0.638 sec</b>.
Processed 117.73 million rows, <b>1.65 GB</b>
(184.50 million rows/s., 2.58 GB/s.)</code>
    </section>

    <section class="slide">
        <h2>Sampling Key</h2>
        <p style="margin-top: -20px;">Requirements:<br/>
   &mdash; must be part of the primary key;</p>
<p>&mdash; must be uniformly distributed in its data type:<br/>
<span style="color: red;">Bad:</span> Timestamp;<br/>
<span style="color: green;">Good:</span> intHash32(UserID);</p>
<p>&mdash; must be lightweight to read and compute:<br/>
<span style="color: red;">Bad:</span> cityHash64(URL);<br/>
<span style="color: green;">Good:</span> intHash32(UserID);</p>
<p>&mdash; should not be placed after fine-grained part of PK:<br/>
<span style="color: red;">Bad:</span> ORDER BY (Timestamp, sample_key);<br/>
<span style="color: green;">Good:</span> ORDER BY (CounterID, Date, sample_key).</p>
    </section>

    <section class="slide">
        <h2>Sampling Key</h2>
        <p>Sampling properties:</p>
        <p>&mdash; sampling is deterministic;</p>
        <p>&mdash; works consistently across different tables;</p>
        <p>&mdash; allows reading less data from disk;</p>
    </section>

    <section class="slide">
        <h2>Sampling Key, Bonus</h2>
        <code style="margin-top: -35px;">SAMPLE 1/10</code>
        <p>&mdash; select a subset of 1/10 of all possible sample keys;</p>
        <code>SAMPLE 1000000</code>
        <p>&mdash; select a subset of (at least) 1,000,000 rows, within each shard;<br/>
        &mdash; you can use the virtual column <b>_sample_factor</b> to determine the relative sampling coefficient;</p>
        <code>SAMPLE 1/10 OFFSET 1/10</code>
        <p>&mdash; select a subset from the second 1/10 of all possible sample keys;</p>
        <code>SET max_parallel_replicas = 3</code>
        <p>&mdash; parallelize the query across multiple replicas of each shard;</p>
    </section>

    <section class="slide">
        <h2>Aggregate Function Combinators</h2>
        <p>-If<br/>-Array<br/>-ForEach<br/>-Merge<br/>-State</p>
        <p>Example: sumIf(x, cond)</p>
    </section>

<section class="slide">
        <h2 style="font-size: 32pt;">Aggregate Function Combinators: -If</h2>

<code>SELECT
  uniq<b>If</b>(UserID, RefererDomain = 'yandex.ru')
    AS users_yandex,
  uniq<b>If</b>(UserID, RefererDomain = 'google.ru')
    AS users_google
FROM test.hits

┌─users_yandex─┬─users_google─┐
│        19731 │         8149 │
└──────────────┴──────────────┘</code>
</section>

<section class="slide">
        <h2 style="font-size: 32pt;">Aggregate Function Combinators: -Array</h2>

<code>SELECT
    uniq(arr),
    uniq<b>Array</b>(arr),
    groupArray(arr),
    groupUniqArray(arr),
    groupArray<b>Array</b>(arr),
    groupUniqArray<b>Array</b>(arr)
FROM
(
    SELECT ['hello', 'world'] AS arr
    UNION ALL
    SELECT ['goodbye', 'world']
)
FORMAT Vertical
</code>
</section>

<section class="slide">
        <h2 style="font-size: 32pt;">Aggregate Function Combinators: -Array</h2>
<code style="font-size: 80%;">Row 1:
──────
uniq(arr):                2
uniq<b>Array</b>(arr):           3
groupArray(arr):          [['hello','world'],['goodbye','world']]
groupUniqArray(arr):      [['hello','world'],['goodbye','world']]
groupArray<b>Array</b>(arr):     ['hello','world','goodbye','world']
groupUniqArray<b>Array</b>(arr): ['goodbye','world','hello']</code>
</section>

    <section class="slide">
        <h2>Aggregate Function Combinators</h2>
        <p>... can be combined with each other</p>
        <p>Example: sumArrayIf, sumIfArray.</p>
    </section>

    <section class="slide">
        <h2>Aggregate Function Combinators</h2>
        <p>... can be combined with each other</p>
        <p>Example: sumForEachStateForEachIfArrayIfState.</p>
    </section>


    <section class="slide">
        <h2 style="line-height: 1.25;">Aggregate Function Computation States Are First-Class Objects</h2>
        <p>The <b>-State</b> combinator &mdash; get the computation state<br/>of an aggregate function;</p>
        <p>Example: uniqState(user_id) AS state;</p>
        <p>&mdash; returns a value of type <b>AggregateFunction(...)</b>;</p>
        <p>&mdash; such values can be saved in tables;</p>
        <p>&mdash; combined together and get the result using <b>-Merge</b>;</p>
        <p>Example: uniqMerge(state) AS result;</p>
    </section>

    <section class="slide">
        <h2 style="font-size: 30pt;">Aggregate Function Computation States</h2>
        <code>SELECT
    avg(x),
    uniq(x)
FROM
(
    SELECT 123 AS x
    UNION ALL
    SELECT 456
)

┌─avg(x)─┬─uniq(x)─┐
│  289.5 │       2 │
└────────┴─────────┘</code>
    </section>

    <section class="slide">
        <h2 style="font-size: 30pt;">Aggregate Function Computation States</h2>
        <code>SELECT
    avg<b>State</b>(x),
    uniq<b>State</b>(x)
FROM
(
    SELECT 123 AS x
    UNION ALL
    SELECT 456
)

┌─avgState(x)─────┬─uniqState(x)─┐
│ C\0\0\0\0\0\0   │ \0▒�P���a�   │
└─────────────────┴──────────────┘</code>
    </section>

    <section class="slide">
        <h2 style="font-size: 30pt;">Aggregate Function Computation States</h2>
        <code style="font-size: 90%;">SELECT
    toTypeName(avgState(x)),
    toTypeName(uniqState(x))
FROM
(
    SELECT 123 AS x
    UNION ALL
    SELECT 456
)
FORMAT Vertical

Row 1:
──────
toTypeName(avgState(x)):  AggregateFunction(avg, UInt16)
toTypeName(uniqState(x)): AggregateFunction(uniq, UInt16)</code>
    </section>

    <section class="slide">
        <h2 style="font-size: 30pt;">Aggregate Function Computation States</h2>
        <code>CREATE TABLE t
(
    users_state AggregateFunction(uniq, UInt64),
    ...
) ENGINE = AggregatingMergeTree ORDER BY ...</code>
        <br />
        <code>SELECT uniqMerge(uniq_state)
FROM t GROUP BY ...</code>

    </section>


    <section class="slide">
        <h2 style="font-size: 30pt;">Aggregate Function Computation States</h2>
        <p>Main use case:</p>
        <p>Incremental data aggregation<br/>
        using the <b>AggregatingMergeTree</b> table engine<br>
        as a MATERIALIZED VIEW.</p>
    </section>

    <section class="slide">
        <h2 style="font-size: 30pt;">How Could We Improve This Feature?</h2>
        <p>&mdash; versioning of aggregate function states;</p>
        <p>&mdash; determine when different aggregate functions have the same state (sumState and sumIfState should be compatible);</p>
        <p>&mdash; add the ability to create an aggregate function state using a regular function (currently you can use the <b>arrayReduce</b> function for this);</p>
        <p>&mdash; add the ability to insert values of AggregateFunction type into tables by passing a tuple of aggregate function arguments;</p>
        <p>&mdash; adaptive index_granularity depending on row thickness;</p>
    </section>


    <section class="slide">
        <h2>Tunable Consistency</h2>
        <p style="margin-top: -30px;">By default, ClickHouse implements:</p>
        <p><b>asynchronous</b>, <b>conflict-free</b>, <b>multi-master</b> replication.</p>
        <p>Asynchronous:</p>
        <p>Client receives confirmation of INSERT after data is written to one replica; replication itself happens asynchronously.</p>
        <p>Replicas can lag and not contain part of the data;</p>
        <p>At any given moment, all replicas may not contain some different parts of the data.</p>
        <p>By default, you only get eventual consistency.</p>
    </section>

    <section class="slide">
        <h2>Tunable Consistency</h2>
        <p style="margin-top: -30px;">You can enable strict consistency (linearizability).</p>
        <code>SET insert_quorum = 2;</code>
        <p><br/>&mdash; each INSERT is confirmed after data is written to a quorum of replicas;</p>
        <p>&mdash; all replicas in the quorum are consistent: they contain data from all previously occurred INSERTs (the sequence of INSERTs is linearized);</p>
        <code>SET select_sequential_consistency = 1;</code>
        <p><br/>&mdash; allows using only confirmed data from consistent replicas for SELECT <br/>(which contain all confirmed INSERTs).</p>
    </section>


    <section class="slide">
        <h2>GROUP BY in External Memory</h2>
        <img style="height: 60%" src="pictures/plain.png"/>
    </section>

    <section class="slide">
        <video style="height: 90%"><source src="video/plain.ogv" type="video/ogg"></video>
    </section>

    <section class="slide">
        <video style="height: 90%"><source src="video/stats.ogv" type="video/ogg"></video>
    </section>

    <section class="slide">
        <h2>GROUP BY in External Memory</h2>
        <p>First, you can simply increase <b>max_memory_usage</b></p>
    </section>

    <section class="slide">
        <video style="height: 90%"><source src="video/raise_mem.ogv" type="video/ogg"></video>
    </section>

    <section class="slide">
        <h1>GROUP BY in External Memory</h1>
        <p>If that's not enough &mdash; enable external aggregation:</p>
        <p><b>max_bytes_before_external_group_by</b></p>
        <p><b>distributed_aggregation_memory_efficient</b></p>
    </section>

    <section class="slide">
        <video style="height: 90%"><source src="video/external.ogv" type="video/ogg"></video>
    </section>


    <section class="slide">
        <h2>Working with Geographic Data</h2>
        <p>&mdash; pointInPolygon;</p>
        <p>&mdash; pointInEllipses;</p>
        <p>&mdash; greatCircleDistance;</p>
        <code>SELECT pointInPolygon((lat, lon),
    [(6, 0), (8, 4), (5, 8), (0, 2), ...])</code>
    </section>

    <section class="slide">
        <h2>Machine Learning Models</h2>
        <code>SELECT modelEvaluate('name', f1, ... fn)
    AS ctr_prediction</code>
        <p><br/><a href='https://events.yandex.ru/lib/talks/5330/'>https://events.yandex.ru/lib/talks/5330/</a></p>
    </section>

    <section class="slide">
        <h2>Machine Learning Models</h2>
        <p>How could we improve this feature?</p>
        <p>&mdash; add simpler regression models;</p>
        <p>&mdash; training models directly in ClickHouse;</p>
        <p>&mdash; online model training;</p>
        <p>&mdash; parameterized models (dictionaries of many models);</p>
    </section>

    <section class="slide">
        <h2 style="line-height: 1">Processing Data Without a Server</h2>
        <p>The <b>clickhouse-local</b> utility</p>
        <code>$ clickhouse-local \
  --input-format=CSV --output-format=PrettyCompact \
  --structure="SearchPhrase String, UserID UInt64" \
  --query="SELECT SearchPhrase, count(), uniq(UserID)
    FROM table \
    WHERE SearchPhrase != '' GROUP BY SearchPhrase \
    ORDER BY count() DESC LIMIT 20" &lt; hits.csv

┌─SearchPhrase────────────┬─count()─┬─uniq(UserID)─┐
│ интерьер ванной комнаты │    2166 │            1 │
│ яндекс                  │    1655 │          478 │
│ весна 2014 мода         │    1549 │            1 │
│ фриформ фото            │    1480 │            1 │
│ анджелина джоли         │    1245 │            1 │
</code>
    </section>

    <section class="slide">
        <h2>Processing Data Without a Server</h2>
        <p>Bonus: ability to process data from a stopped clickhouse-server.</p>
    </section>

    <section class="slide">
        <h2>Processing Data Without a Server</h2>
        <p>How could we make this feature better?</p>
        <p>&mdash; add more supported formats for Date and DateTime in text form;</p>
        <p>&mdash; add Avro, Parquet formats;</p>
        <p>&mdash; flexible settings for CSV format;</p>
        <p>&mdash; "template" and "regexp" formats for trash data;</p>
    </section>

    <section class="slide">
        <h2>Cross-Cluster Copying</h2>
        <p><b>clickhouse-copier</b></p>
        <p>&mdash; runs on arbitrary servers in any number of instances;</p>
        <p>&mdash; tasks are coordinated through ZooKeeper;</p>
        <p>&mdash; unit of work &mdash; one partition on one shard of the resulting table;</p>
        <p>&mdash; copying is performed reliably and fault-tolerantly;</p>
        <p>&mdash; configurable limit on network bandwidth usage and number of concurrent tasks;</p>
    </section>

    <section class="slide">
        <h2>Cross-Cluster Copying</h2>
        <p>Example in Yandex.Metrica</p>
        <p>&mdash; 538 -> 240 servers;</p>
        <p>&mdash; sharding by CounterID -> sharding by UserID;</p>
        <p>&mdash; lz4 -> zstd;</p>
    </section>

    <section class="slide">
        <h2>.</h2>
    </section>

    <section class="slide">
        <p style="margin-top: 50px;">Web site: <a href="https://clickhouse.com/">https://clickhouse.com/</a></p>
        <p>Google groups: <a href="https://groups.google.com/forum/#!forum/clickhouse">https://groups.google.com/forum/#!forum/clickhouse</a></p>
        <p>Maillist: clickhouse-feedback@yandex-team.com</p>
        <p>Telegram chat: <a href="https://telegram.me/clickhouse_ru">https://telegram.me/clickhouse_ru</a> (more than 1500 participants) and <a href="https://telegram.me/clickhouse_en">https://telegram.me/clickhouse_en</a></p>
        <p>GitHub: <a href="https://github.com/ClickHouse/ClickHouse/">https://github.com/ClickHouse/ClickHouse/</a></p>
        <p>Twitter: <a href="https://twitter.com/ClickHouseDB">https://twitter.com/ClickHouseDB</a></p>

        <p>+ meetups. Moscow, Saint Petersburg, Novosibirsk, Yekaterinburg, Minsk, Nizhny Novgorod, Berlin, Palo Alto, Beijing, Sunnyvale, San Francisco...</p>
    </section>

    <div class="progress"></div>
    <script src="shower/shower.min.js"></script>

    <!--Video plugin-->
    <link rel="stylesheet" href="shower/shower-video.css">
    <script src="shower/shower-video.js"></script>
    <!--/Video plugin-->
</body>
</html>
