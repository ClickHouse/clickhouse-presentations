<!DOCTYPE html>
<html lang="en">
<head>
    <title>Efficient Use of ClickHouse</title>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta property="og:image" content="pictures/preview.jpg">
    <meta property="og:title" content="Efficient Use of ClickHouse">
    <meta property="og:type" content="website">
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:image" content="pictures/preview.jpg">
    <link rel="stylesheet" href="shower/themes/clickhouse/styles/styles.css">
    <style>
        pre { line-height: 1 }
    </style>
</head>
<body class="shower list">
    <header class="caption">
        <h1>Efficient Use of ClickHouse</h1>
    </header>

    <section class="slide" id="cover" style="background: #FFF url('pictures/title.png') no-repeat; background-size: 100%;">
        <h1 style="margin-top: 200px; font-size: 32pt;">Efficient Use of<br/>ClickHouse</h1>
    </section>

    <section class="slide">
        <h2>About Me</h2>
        <p>Alexey, ClickHouse developer.</p>
    </section>

    <section class="slide">
        <h2 style="font-size: 28pt;">I Won't Explain What ClickHouse Is</h2>
        <p>1. I'm tired of it.</p><p>2. Everyone already knows.</p>
    </section>

    <section class="slide">
        <h2 style="font-size: 28pt;">How to Use ClickHouse Incorrectly</h2>
        <p>Common mistakes we've encountered in practice.</p>
        <ul>
            <li>which scenarios are suboptimal for ClickHouse;</li>
            <li>architectural reasons for this suboptimality;</li>
            <li>and what are the best practices to avoid these issues.</li>
        </ul>
    </section>

    <section class="slide">
        <h2 style="font-size: 24pt;">1. Too Frequent Inserts with Small Batch Sizes</h2>
        <p>Example:
        <br/>
        <br/>Table hits: 105 columns,
        <br/>row size in uncompressed binary format &mdash; 718 bytes.</p>
        <p>
            Batch size 1000000 rows:<br/>
            <br/>
            Insert into MergeTree table<br/>
            &mdash; <b>506,650</b> rows per second;<br/><br/>
            Insert into ReplicatedMergeTree table with two replicas<br/>
            &mdash; <b>393,940</b> rows per second;<br/><br/>
            Insert into ReplicatedMergeTree table<br/>
            with two replicas and insert_quorum = 2
            <br/>&mdash; <b>259,660</b> rows per second;<br/>
        </p>
    </section>

    <section class="slide">
        <h2 style="font-size: 24pt;">1. Too Frequent Inserts with Small Batch Sizes</h2>
        <p>Example:
        <br/>
        <br/>Table hits: 105 columns,
        <br/>row size in uncompressed binary format &mdash; 718 bytes.</p>
        <p>
            Batch size &mdash; one row:<br/>
            <br/>
            Insert into MergeTree table<br/>
            &mdash; <b>59</b> rows per second;<br/><br/>
            Insert into ReplicatedMergeTree table with two replicas<br/>
            &mdash; <b>6</b> rows per second;<br/><br/>
            Insert into ReplicatedMergeTree table<br/>
            with two replicas and insert_quorum = 2
            <br/>&mdash; <b>2</b> rows per second;<br/>
        </p>
    </section>

    <section class="slide">
        <h2 style="font-size: 24pt;">1. Too Frequent Inserts with Small Batch Sizes</h2>
        <p>&mdash; 10,000 .. 100,000 times slower.</p>
        <p><br/>Why does this happen?</p>
        <p>This is a limitation of ClickHouse.</p>
        <p>MergeTree tables don't have a log and memtable.</p>
        <p>Data is immediately written to the file system,<br/>arranged by columns.</p>
    </section>

    <section class="slide">
        <h2 style="font-size: 24pt;">How to Do It Right?</h2>
        <p>Method 1.</p><p>Use a distributed queue (Kafka).</p>
        <p>Extract data from the queue in batches once per second<br/>and write to ClickHouse.</p>

        <p><br/>Disadvantages:</p>
        <p>&mdash; cumbersome construction;</p>
        <p>&mdash; another complex distributed system.</p>
    </section>

    <section class="slide">
        <h2 style="font-size: 24pt;">How to Do It Right?</h2>
        <p>Method 2.</p><p>Write logs to a file.</p>
        <p>Once per second, rename the file and open a new one.</p>
        <p>A separate process takes the oldest file and inserts it into ClickHouse.</p>

        <p><br/>Disadvantages:</p>
        <p>&mdash; possibility of data loss in case of hardware failure.</p>
    </section>

    <section class="slide">
        <h2 style="font-size: 24pt;">How to Do It Right?</h2>
        <p>Method 3.</p><p>Server process accumulates a batch of logs in an internal buffer.</p>
        <p>Periodically, the accumulated buffer is swapped with an empty one<br/>
        and a separate thread writes the accumulated data to ClickHouse.</p>

        <p><br/>Disadvantages:</p>
        <p>&mdash; data loss if the process is killed with kill -9;</p>
        <p>&mdash; data loss or OOM if unable to write to ClickHouse;</p>
    </section>

    <section class="slide">
        <h2 style="font-size: 24pt;">How to Do It Right?</h2>
        <p>Method 4.</p><p>Server process keeps an open connection to ClickHouse,<br/>
        in which an INSERT query is made and rows are sent one by one <br/>
        or in small batches with Transfer-Encoding: chunked.</p>

        <p>Periodically, the data stream ends<br/>and a new INSERT query is executed.</p>
        <p>In this case, ClickHouse will buffer the data<br/> on its side.</p>

        <p>Disadvantages:</p>
        <p>&mdash; data loss if the process is killed with kill -9;</p>
    </section>

    <section class="slide">
        <h2 style="font-size: 24pt;">How to Do It Right?</h2>
        <p>Method 5.</p>
        <p>https://github.com/nikepan/clickhouse-bulk</p>

        <p>&mdash; intermediate server for grouping INSERTs into batches;</p>
    </section>

    <section class="slide">
        <h2 style="font-size: 24pt;">How to Do It Right?</h2>
        <p>Method 6.</p><p>Buffer tables.</p>
        <p><br/>Advantages:</p>
        <p>&mdash; very easy to start using;</p>
        <p><br/>Disadvantages:</p>
        <p>&mdash; doesn't completely solve the problem, batches are still needed;</p>
        <p>&mdash; introduces other problems (no log, possible data loss).</p>
    </section>

    <section class="slide">
        <h2 style="font-size: 24pt;">Bonus</h2>
        <p>Support for direct import from Kafka inside ClickHouse.</p>
    </section>

    <section class="slide">
        <h2 style="font-size: 24pt;">More Pitfalls</h2>
        <p>&mdash; inserting data in VALUES format with computed expressions;</p>
        <p>&mdash; inserting data belonging to a large<br/>number of partitions in each batch.</p>
    </section>


    <section class="slide">
        <h2 style="font-size: 24pt;">2. String Typing</h2>
        <p>Example:</p>
        <p>Country String,
        <br/>City String,
        <br/>Browser String,
        <br/>IPAddress String,
        <br/>...
        </p>
    </section>

    <section class="slide">
        <h2 style="font-size: 24pt;">How Bad Is This?</h2>
        <p>Strings compress almost as well as numeric identifiers.</p>
        <p>There's almost no difference in disk and IO.<br/>
        But during query processing, the CPU difference is at least several times.<br/>
        Example:</p>
        <p>SELECT uniqExact(IPAddress) FROM ip_address_num<br/>
        &mdash; <b>0.073</b> sec, <b>137.79</b> million rows/s.</p>

        <p>SELECT uniqExact(IPAddress) FROM ip_address_string<br/>
        &mdash; <b>0.265</b> sec, <b>37.70</b> million rows/s.</p>

        <p>SELECT formatReadableSize(sum(data_compressed_bytes)) FROM system.columns WHERE table = 'ip_address_num'<br/>
        &mdash; <b>30.92</b> MiB vs. <b>43.65</b> MiB.</p>
    </section>

    <section class="slide">
        <h2 style="font-size: 24pt;">How to Do It Right?</h2>

        <p>1. If there are few different values,<br/>just store numeric identifiers.</p>
        <p>Converting to strings and back can be done on the application side.</p>
        <p>This is a typical practice applicable to almost all DBMSs.</p>

        <p>Example:</p>

        <p style="color: red;">Region String<br/>'Moscow and Moscow Region'</p>
        <p style="color: green;">RegionID UInt32<br/>250</p>
    </section>

    <section class="slide">
        <h2 style="font-size: 24pt;">How to Do It Right?</h2>

        <p>2. If there are few different values, you can use the Enum data type.</p>
        <p>Enum works as efficiently as a numeric identifier.</p>
        <p>Disadvantage: sometimes you need to do ALTER.<br/>ALTER is free, but waits for running queries on a lock.</p>

        <p>Example:</p>

        <p style="color: red;">DeviceType String<br/>'mobile'</p>
        <p style="color: green;">DeviceType Enum8('desktop' = 1, 'mobile' = 2, 'tablet' = 3, 'tv' = 4)</p>
    </section>

    <section class="slide">
        <h2 style="font-size: 24pt;">How to Do It Right?</h2>

        <p>3. If there aren't too many different values,<br/>just store numeric identifiers.</p>
        <p>For convenience, you can connect external dictionaries.
        <br/>From ClickHouse, MySQL, Mongo, Postgres, ODBC, file, HTTP server...</p>

        <p>Example: advertising campaigns and banners in Yandex.Direct.</p>
        <p>ClickHouse stores CampaignID UInt32, BannerID UInt64.</p>
        <p>Dictionaries are connected from MySQL.<br/>Campaigns &mdash; always in memory. Banners &mdash; cache dictionary.</p>

        <p>Bonus: easy to change strings.</p>

        <p>Features: cache dictionary from MySQL<br/>works normally only with hit rate ~ 100%.</p>
    </section>

    <section class="slide">
        <h2 style="font-size: 24pt;">How to Do It Right?</h2>

        <p>4. If you can't do auto-increment, you can hash.</p>
        <p>When using a 64-bit hash, there will be collisions<br/>(p ~ 1/2 at N ~ 1000000000).</p>
        <p>To avoid worrying, add<br/>client identifier to the dictionary key.</p>
        <p>Bonus &mdash; for many operations, hashes alone are sufficient.</p>
    </section>

    <section class="slide">
        <h2 style="font-size: 24pt;">How to Do It Right?</h2>

        <p>5. Short strings are fine to store as is.</p>
        <p>Example:</p>
        <p style="color: green;">URLDomain String<br/>'yandex.ru'</p>
        <p style="color: green;">BrowserLanguage String<br/>'ru'</p>
    </section>

    <section class="slide">
        <h2 style="font-size: 24pt;">How to Do It Right?</h2>

        <p>6. If the set of strings is large and unlimited,<br/>and a significant portion is unique.</p>
        <p>Example: URL, Referer, SearchPhrase, Title.</p>
        <p>Then making dictionaries is completely pointless and contraindicated.</p>
        <p style="color: green;">Store as is!</p>
        <p>Don't try to put them in a separate table and normalize the data.</p>
    </section>

    <section class="slide">
        <h2 style="font-size: 24pt;">How to Do It Right?</h2>

        <p>7. Sometimes it's reasonable to store redundant columns with extracted data.</p>

        <p>Example: there's a URL column.<br>
        Add another URLDomain column.</p>

        Example:</p>
        <p>SELECT <span style="color: red;">domain(URL)</span> AS k, count() AS c FROM test.hits GROUP BY k<br/>
        &mdash; <b>0.166</b> sec. Processed 762.68 MB (53.60 million rows/s., 4.61 GB/s.)</p>

        <p>SELECT <span style="color: green;">URLDomain</span> AS k, count() AS c FROM test.hits GROUP BY k<br/>
        &mdash; <b>0.067</b> sec. Processed 188.06 MB (132.44 million rows/s., 2.81 GB/s.)</p>

        <p>On disk: URL - <b>126.80</b> MiB, URLDomain - <b>5.61</b> MiB.</p>
    </section>

    <section class="slide">
        <h2 style="font-size: 24pt;">How to Do It Right?</h2>

        <p>8. Use the correct data types.</p>

        <p style="color: red;">ClientIP String '5.255.255.5'<br/>
            ClientIPv6 String '2a02:6b8:a::a'<br/>
            DeviceID String 'c9792d6e-ab40-42e6-b171-aa872e880dbd'<br/>

        <p style="color: green;">ClientIP UInt32<br/>
            ClientIPv6 FixedString(16)<br/>
            DeviceID FixedString(16)</p>
    </section>

    <section class="slide">
        <h2 style="font-size: 24pt;">How to Do It Right?</h2>

        <p>9. Sometimes strings are better preprocessed in advance.</p>

        <p style="color: red;">BrowserVersion String<br/>
        '12.3.67542.29'<br/>
        <br/>splitByChar('.', BrowserVersion)[1]</p>

        <p style="color: green;">BrowserVersion1 UInt8<br/>
        BrowserVersion2 UInt16<br/>
        BrowserVersion3 UInt32<br/>
        BrowserVersion4 UInt16</p>
    </section>

    <section class="slide">
        <h2 style="font-size: 24pt;">How to Do It Right?</h2>

        <p>10. Very long strings and very long arrays.</p>

        <p>Don't store them in ClickHouse at all.</p>
        <p>Two options:</p>
        <p>1. Truncate on insert.</p>
        <p>Example: in one service, only<br/>the first 512 user event parameters are stored;</p>
        <p>2. Store externally, and in ClickHouse &mdash; hashes.</p>
    </section>

    <section class="slide">
        <h2 style="font-size: 24pt;">Bonus:</h2>

        <p>You can load data into a Log table with String fields,<br/>
        then analyze them in ClickHouse,<br/>and determine the correct types before loading further.</p>
    </section>



    <section class="slide">
        <h2 style="font-size: 24pt;">3. Large Number of Small Tables.</h2>

        <pre>:) SHOW TABLES

┌─name──────┐
│ stats_0   │
│ stats_1   │
│ stats_2   │
│ stats_3   │
│ stats_4   │
│ stats_5   │
│ stats_6   │
│ stats_7   │
│ stats_8   │
│ stats_9   │
│ stats_10  │
  ...
│ stats_999 │
└───────────┘
</pre>
    </section>

    <section class="slide">
        <h2 style="font-size: 24pt;">Reasons for Using:</h2>

        <p>Old habits from MySQL experience:</p>
        <p>&mdash; lack of clustered primary key in MyISAM tables in MySQL;</p>
        <p>&mdash; simpler maintenance operations with small tables;</p>
        <p>&mdash; microsharding.</p>
    </section>

    <section class="slide">
        <h2 style="font-size: 24pt;">Why You Shouldn't Do This in ClickHouse?</h2>

        <p>1. Data in the table is ordered by primary key.</p>
        <p>Reading a range is efficient regardless of the total<br/>amount of data in the table. Primary key is clustered.</p>

        <p>2. Table already contains internal partitioning.</p>
        <p>Each table is a rather bulky object.</p>
        <p>For each table, some background merge work<br/>must be constantly performed.</p>

    </section>

    <section class="slide">
        <h2 style="font-size: 24pt;">Why You Shouldn't Do This in ClickHouse?</h2>

        <p>3. No problems with maintenance operations.</p>
        <p>ALTER ADD/DROP COLUMN executes in O(1)</p>
        <p>Table consists of immutable pieces and recovery after failure is trivial.</p>

        <p>4. ClickHouse is about throughput.</p>
        <p>Processing time for 10 rows is about the same as 10,000 rows.</p>
        <p>If your tables contain less than 10,000 rows,<br/>then smaller doesn't matter.</p>
    </section>

    <section class="slide">
        <h2 style="font-size: 24pt;">How to Do It Right?</h2>

        <p><span style="color: green;">One big table!</span> Don't be afraid.</p>
        <p>Example: we store a table with Graphite metrics,<br/>
        on one server &mdash; 2,000,000,000,000 rows.</p>

        <p>Bonus:</p>

        <p>Support for arbitrary partitioning key in ClickHouse.</p>
        <p>If you still need many small tables without an index<br>&mdash; use StripeLog.</p>
    </section>



    <section class="slide">
        <h2 style="font-size: 24pt;">4. Microsharding.</h2>

        <p>A reasonable approach to sharding. From another world.</p>
        <p>Don't use this approach in ClickHouse.</p>

        <p>The essence &mdash; data is divided into 1000 virtual shards<br/>in any convenient way.</p>
        <p>Each virtual shard is mapped to a set of servers arbitrarily, and the mapping is stored separately.</p>
        <p>On each physical server, several instances<br/>&mdash; in different databases or on different ports.</p>
        <p>Reason &mdash; simplify the task of data rebalancing.</p>
    </section>

    <section class="slide">
        <h2 style="font-size: 24pt;">4. Microsharding.</h2>

        <p>Why you shouldn't use this for ClickHouse?</p>
        <p>1. If data is located in different ClickHouse instances, <br/>
        then query processing by different instances requires<br/><b>interprocess communication via TCP</b>.<br/>
        Even over localhost, this is much slower than in-process.</p>

        <p>2. With a single instance, threads process data as they can, work is evenly distributed across threads.<br/>
        With different instances, one instance will finish work before others &mdash; <b>tail latency</b>.</p>

        <p>3. Even if tables are located in one ClickHouse instance,<br/>
        you get problems due to a large number of small tables.<br/>
        When combining tables with a Merge table, <b>some optimizations are disabled</b>.</p>
    </section>


    <section class="slide">
        <h2 style="font-size: 24pt;">5. Too Much Preaggregation.</h2>

        <p>Preaggregation is normal. Even in ClickHouse.</p>
        <p>But too granular preaggregation is pointless.</p>

        <p>Example*:</p>
        <p>SummingMergeTree(EventDate,<br/>
(OrderID, EventDate, CampaignID, BannerID, AdID, TypeID,<br/>
DisplayID, ContextID, TargetType, AbcID, IsGood, CostType, PageID),<br/>
8192)</p>
        <p style="font-size: 14pt;">* column names changed.</p>
    </section>

    <section class="slide">
        <h2 style="font-size: 24pt;">5. Too Much Preaggregation.</h2>

        <p>Disadvantages:</p>
        <p>&mdash; data volume decreases, but only by several times compared to non-aggregated data;</p>
        <p>&mdash; developers constantly ask to extend the primary key, but ClickHouse has no support for ALTER PRIMARY KEY;</p>
        <p>&mdash; among aggregates, only sums are calculated (could be worked around with AggregatingMergeTree)</p>
        <p>&mdash; suitable only for primitive reports, not for analytics.</p>

        <p>For ClickHouse, non-aggregated data is the best scenario.<br/>
        Preaggregation can also be used for simple cases.</p>
    </section>



    <section class="slide">
        <h2 style="font-size: 24pt;">6. O(N) Queries from Script in Loop.</h2>

        <p style="color: red; font-size: 16pt;">
             SELECT count() FROM hits WHERE URL = 'https://yandex.ru/?cid=1764026490'
        <br/>SELECT count() FROM hits WHERE URL = 'https://yandex.ru/?cid=132741461'
        <br/>SELECT count() FROM hits WHERE URL = 'https://yandex.ru/?cid=4897535466'
        <br/>SELECT count() FROM hits WHERE URL = 'https://yandex.ru/?cid=1301256137'
        <br/>SELECT count() FROM hits WHERE URL = 'https://yandex.ru/?cid=2149858559'
        <br/>SELECT count() FROM hits WHERE URL = 'https://yandex.ru/?cid=544614111'
        <br/>...</p>

        <p style="color: green;">SELECT URL, count() FROM hits WHERE URL IN (...) GROUP BY URL</p>
    </section>

    <section class="slide">
        <h2 style="font-size: 24pt;">Why Are Many Queries Bad?</h2>

        <p>If the index isn't used, then each query<br/> is a separate pass through the data;</p>
        <p>If the index is used, then each query<br/>reads slightly more data than needed,
        <br/>for example, reads 100,000 rows instead of one row<br/>&mdash; again many passes through the same data.</p>

        <p>Can be rewritten as one query.</p>
    </section>

    <section class="slide">
        <h2 style="font-size: 24pt;">6. O(N) Queries from Script in Loop.</h2>

        <p>Bonuses:</p>
        <p>&mdash; in ClickHouse, you don't need to worry about<br/>passing megabytes of data in the IN section;</p>
        <p>&mdash; in ClickHouse, the index always works no slower than a full scan;</p>
        <p>Features:</p>
        <p>&mdash; IN (SELECT ...) doesn't use the index;</p>
        <p>&mdash; in distributed query processing,<br/>query text is transmitted without compression.</p>
    </section>

    <section class="slide">
        <h2 style="font-size: 24pt;">6. O(N) Queries from Script in Loop.</h2>

        <p>What if queries come from an API?</p>
        <p>&mdash; limit API users:<br/>quotas, adaptive throttling.</p>

        <p>Bonus: you can set up quotas per user in ClickHouse by passing quota_key.</p>

        <p style="color: green;">If the API is accessible externally, quotas are mandatory!</p>
    </section>


    <section class="slide">
        <h2 style="font-size: 24pt;">7. Manual Replication.</h2>

        <p>What is this?</p>
        <p>You write the same data identically<br/> to completely independent servers.</p>

        <p>Advantages:<br/>
        &mdash; no ZooKeeper in infrastructure;<br/>
        &mdash; ease of use, integration into existing pipeline.</p>

        <p>Disadvantages:<br/>
        &mdash; need to restore replicas manually;<br/>
        &mdash; due to errors and non-deterministic behavior on the application side, data diverges during operation;<br/>
        &mdash; you can't determine which copy of data is correct.</p>
    </section>

    <section class="slide">
        <h2 style="font-size: 24pt;">7. Manual Replication.</h2>

        <p>Bonus:</p>
        <p>In ClickHouse, it's easy to create a Distributed table,<br/>that points to "manual" replicas.</p>
        <p>Using load_balancing = 'nearest_hostname'<br/>you can avoid flaps with diverging replicas.</p>

        <p>Correct solution:<br/>Use ReplicatedMergeTree.</p>
        <p>Advantages:<br/>
        &mdash; recovery after failure;<br/>
        &mdash; replicas monitor data consistency themselves.</p>
    </section>


    <section class="slide">
        <h2 style="font-size: 24pt; line-height: 1.5;">8. Using Primitive Table Engines<br/> for Primary Data.</h2>

        <p>For primary data &mdash; use the MergeTree family.</p>

        <p>
        &mdash; table doesn't have to contain Date<br/>
        &mdash; you can use Date DEFAULT '2000-01-01',<br/>
        &mdash; or tables without partitioning.</p>

        <p>Reasons:</p>
        <p>
        &mdash; sorting by key, index;<br/>
        &mdash; atomic insert;<br/>
        &mdash; no locks with concurrent INSERT and SELECT;<br/>
        &mdash; immutable data structures on disk;<br/>
        &mdash; replication;</p>
    </section>

    <section class="slide">
        <h2 style="font-size: 24pt;">8. Using Primitive Table Engines.</h2>

        <p>For one-time loading of small-medium volumes<br/>&mdash; <b>Log</b>.</p>
        <p>For temporary batches for intermediate processing<br/>&mdash; <b>StripeLog</b>, <b>TinyLog</b>.</p>
        <p>For small volume of temporary data<br/>&mdash; <b>Memory</b>.</p>
    </section>


    <section class="slide">
        <h2 style="font-size: 24pt; line-height: 1.5;">9. Over-Normalized Data.<br/> JOIN with High Cardinality.</h2>

        <p>Example:</p>

        <pre>SELECT url, count() FROM stats
    ANY INNER JOIN urls USING (url_id)</pre>

        <p>Why is this bad?</p>

        <p>ClickHouse only supports Hash JOIN. No support for Merge JOIN.</p>

        <p>Hash JOIN is random access in memory.<br/>
        For high cardinality, worse than reading data inplace.</p>

        <p>Solution: for joined data with high cardinality,<br/>it's better to store it in the main table.</p>
    </section>


    <section class="slide">
        <h2 style="font-size: 24pt;">More Antipatterns</h2>
        <p>10. Using OPTIMIZE queries to<br/>simulate UPDATE functionality.</p>
        <p>11. Using distributed JOIN<br/>with large right table.<br/>ClickHouse poorly composes query plan for distributed JOINs.</p>

        <h2 style="font-size: 24pt;">Bad, but Sometimes Ok</h2>

        <p>12. Using ClickHouse only to<br/>read data back with SELECT *.</p>
        <p>13. Performing excessively heavy computations<br/>inside ClickHouse (high cycle/byte value).</p>
    </section>


    <section class="slide">
        <h2>Community</h2>
        <p>Website: <a href="https://clickhouse.com/">https://clickhouse.com/</a></p>
        <p>Google groups: <a href="https://groups.google.com/forum/#!forum/clickhouse">https://groups.google.com/forum/#!forum/clickhouse</a></p>
        <p>Mailing list: clickhouse-feedback@yandex-team.com</p>
        <p>Telegram chat: <a href="https://telegram.me/clickhouse_en">https://telegram.me/clickhouse_en</a> and <a href="https://telegram.me/clickhouse_ru">https://telegram.me/clickhouse_ru</a> (already 1041 members)</p>
        <p>GitHub: <a href="https://github.com/ClickHouse/ClickHouse/">https://github.com/ClickHouse/ClickHouse/</a></p>
        <p>+ meetups. Moscow, St. Petersburg, Novosibirsk, Yekaterinburg, Minsk, Berlin... Next: Nizhny Novgorod, Moscow.</p>
    </section>

    <section class="slide">
        <h2>Thank You!</h2>
    </section>


    <div class="progress"></div>
    <script src="shower/shower.js"></script>
</body>
</html>
